bnb_config:
  load_in_4bit: True
  bnb_4bit_use_double_quant: True
  bnb_4bit_quant_type: nf4

lora_config:
  r: 8
  lora_alpha: 16
  target_modules:
  - q_proj
  - up_proj
  - o_proj
  - k_proj
  - down_proj
  - gate_proj
  - v_proj
  lora_dropout: 0.1
  bias: none
  task_type: CAUSAL_LM

model: yanolja/EEVE-Korean-10.8B-v1.0
auth_token: 

training_args:
  num_train_epochs: 10
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  evaluation_strategy: steps
  save_strategy: steps
  save_total_limit: 5
  save_steps: 50
  eval_steps: 50
  logging_steps: 50
  learning_rate: 0.00001
  weight_decay: 0.01
  seed: 42
  fp16: True    
  load_best_model_at_end: True
  report_to: wandb

